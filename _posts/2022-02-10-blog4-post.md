---
layout: post
title: My Blog Post 4
---

In this blog post, I'll write a tutorial on a simple version of the *spectral clustering* algorithm for clustering data points. Each of the below parts will pose to one or more specific tasks.


### Notation

In all the math below: 

- Boldface capital letters like $$\mathbf{A}$$ refer to matrices (2d arrays of numbers). 
- Boldface lowercase letters like $$\mathbf{v}$$ refer to vectors (1d arrays of numbers). 
- $$\mathbf{A}\mathbf{B}$$ refers to a matrix-matrix product (`A@B`). $$\mathbf{A}\mathbf{v}$$ refers to a matrix-vector product (`A@v`). 

## Introduction

Before learning what is *spectral clustering*, which is an important tool for identifying meaningful parts of data sets with complex structure, let me begin with an example where we *don't* need spectral clustering.


```python
import numpy as np
from sklearn import datasets
from matplotlib import pyplot as plt
```


```python
n = 200
np.random.seed(1111)
X, y = datasets.make_blobs(n_samples=n, shuffle=True, random_state=None, centers = 2, cluster_std = 2.0)
plt.scatter(X[:,0], X[:,1])
```

![output_2_1.png]({{ site.baseurl }}/images/output_2_1.png)
    


*Clustering* refers to the task of separating this data set into the two natural "blobs." K-means is a very common way to achieve this task, which has good performance on circular-ish blobs like these: 


```python
from sklearn.cluster import KMeans
km = KMeans(n_clusters = 2)
km.fit(X)

plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x29165cdbdf0>




    
![output_4_1.png]({{ site.baseurl }}/images/output_4_1.png)
    


### Harder Clustering

That was all well and good, but what if our data is "shaped weird"? 


```python
np.random.seed(1234)
n = 200
X, y = datasets.make_moons(n_samples=n, shuffle=True, noise=0.05, random_state=None)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x29165d5b3a0>




    
![output_6_1.png]({{ site.baseurl }}/images/output_6_1.png)
    


We can still make out two meaningful clusters in the data, but now they aren't blobs but crescents. As before, the Euclidean coordinates of the data points are contained in the matrix `X`, while the labels of each point are contained in `y`. Now k-means won't work so well, because k-means is, by design, looking for circular clusters. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x29165dbafa0>




    
![output_8_1.png]({{ site.baseurl }}/images/output_8_1.png)
    


As can be seen above, the two crescents are not clusteded correctly using `K-means`. Luckly, there is other type of clustering named `spectral clustering` that can do it correctly. And in the next part, I will show how I derive and implement it:

## Part A

To perform spectral clustering, the first thing we need to do is constructing is a *similarity matrix* $$\mathbf{A}$$ of an inputted (2d `np.ndarray`) matrix with a square shape `(n, n)`.

When constructing the similarity matrix, it also requires a parameter `epsilon` as the distance threshold which help cluster data into two groups. To make it more specific, I'll let all the entry `A[i,j]` equal to `1` if `X[i]` (the coordinates of data point `i`) is within distance `epsilon` of `X[j]` (the coordinates of data point `j`), and `0` if `X[i]` (the coordinates of data point `i`) is out of distance `epsilon` of `X[j]` (the coordinates of data point `j`).

Also, since the similarity matrix has one feature that all the **diagonal entries `A[i,i]`** are equal to zero. To do that, we can apply a function `fill_diagonal()` from `numpy` in order to set the values of the diagonal of a matrix.

#### Note

Actually, it is doable to compute all the pairwise distances manually using a `for`-loop by comparing `(X[i] - X[j])**2` and `epsilon**2` for each choice of `i` and `j`. However, this methods is not efficient. In fact, there is a easy way to do. By using a built-in function from `sklearn` named `pairwise_distance`, we cannot only get all the pairwise distances, but also collect them into a well-structured matrix that we want!

For this part, I'll use `epsilon = 0.4` as our inital distance threshold. Here's our function `construct_similarity_matrix(X, epsilon = 0.4)` that help us constructing the similarity matrix. Of course, the assumption of this function is to start with a 2d-array matrix, X, as mentioned above.


```python
from sklearn.metrics import pairwise_distances

def construct_similarity_matrix(X, epsilon = 0.4):
    """
    This function assumes the input matrix X is a 2d-array.
    Its output is a similarity matrix A constructed by X.
    To construct A, we first compute all the pairwise distances using sklearn.metrics.pairwise_distances.
    Then if the distances < esilon, make it = 1 or 0 otherwise.
    Lastly, make the diagonal entries equal to 0.
    """
    # compute all the pairwise distances and collect them into an appropriate matrix
    A = pairwise_distances(X)
    # if the pairwise distances is less than esilon, make it equal to 1 and 0 otherwise
    A = (A < epsilon) * 1.0
    # make the diagonal entries equal to 0
    np.fill_diagonal(A, 0)
    return A
```


```python
A = construct_similarity_matrix(X, 0.4)
# demonstration
A
```




    array([[0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 0., 0.],
           [0., 0., 0., ..., 0., 1., 0.],
           ...,
           [0., 0., 0., ..., 0., 1., 1.],
           [0., 0., 1., ..., 1., 0., 1.],
           [0., 0., 0., ..., 1., 1., 0.]])



From above, applying our function with X and 0.4 as our epsilon, we have our similarity matrix $\mathbf{A}$.

## Part B

Having our simiarity matrix `A` is only our first appoarch. We now pose the task of clustering the data points in `X` as the task of partitioning the rows and columns of `A`.

To help better understand the mathmatcial terms before implementing, here's some of the notations with a simple explanation:

- *degree* of $$i$$: $$d_i = \sum_{j = 1}^n a_{ij}$$, which is the $$i$$th row-sum of $$\mathbf{A}$$.

- $$C_0$$ and $$C_1$$: the two clusters of the data points. One example is that if `y[i] = 1`, then point `i` (and therefore row $i$ of $$\mathbf{A}$$) should be labeled as an element of cluster $$C_1$$.  

Here, let me introduce the function of the *binary norm cut objective* of a matrix $$\mathbf{A}$$:

$$N_{\mathbf{A}}(C_0, C_1)\equiv \mathbf{cut}(C_0, C_1)\left(\frac{1}{\mathbf{vol}(C_0)} + \frac{1}{\mathbf{vol}(C_1)}\right)\;.$$

Although the above function looks a bit complicated, the following expression might be useful to help understand:

- $$\mathbf{cut}(C_0, C_1) \equiv \sum_{i \in C_0, j \in C_1} a_{ij}$$, which is the number of nonzero entries in $$\mathbf{A}$$ that relate points in cluster $$C_0$$ to points in cluster $$C_1$$.
- $$\mathbf{vol}(C_*) \equiv \sum_{i \in C_*}d_i$$, where $$d_i = \sum_{j = 1}^n a_{ij}$$ is the *degree* of row $$i$$ (the total number of all other rows related to row $$i$$ through $$A$$).

  Note: the `*` here is either 0 or 1. One example is that the *volume* of cluster $$C_1$$ is a measure of the size of all the data points that are elements of cluster $$C_1$$. 

A pair of clusters $$C_0$$ and $$C_1$$ is considered to be a "good" partition of the data when $$N_{\mathbf{A}}(C_0, C_1)$$ is small. To see why, let's look at each of the two factors in this objective function separately. 


#### B.1 The Cut Term

After learning what the cut term $$\mathbf{cut}(C_0, C_1)$$ is, we then can write a function called `cut(A,y)`. As can be seen below, I compute the cut term by summing up the entries `A[i,j]` for each pair of points `(i,j)` in different clusters.


```python
def cut(A, y):
    '''
    This function assumes that the input matrix A contains information about which points are near
    Its output is the cut term, which is the number of nonzero entries in A that relate points
    in cluster C_0 to points in cluster C_1.
    We can compute it by summing up the entries A[i,j] for each pair of points (i,j) in different clusters.
    '''
    # initialize the cut term = 0
    cut = 0
    # have one entry travel around all other entries
    for row in range(A.shape[0]):
        for col in range(A.shape[1]):
            # if the two points are in different clusters
            if y[row] != y[col]:
                # sum up the entires A[i,j]
                cut += A[row, col]
    # return the cut term
    return cut
```

Once the function is defined correctly, we can quickly apply it with the true clusters `y`. In order to see how good our result is, I'll generate a random vector of random labels of length `n`, with each label equal to either 0 or 1. Before comparing with the cut objective for both the true labels and the random labels, my expectation here would be the cut objective for the true labels is *much* smaller than the cut objective for the random labels.


```python
# generate a random vector of either 0 or 1 with the same length of y
ran_y = np.random.randint(0, 2, 200)
cut(A, y), cut(A, ran_y)
```




    (26.0, 2300.0)



Indeed, the results above matches my expectation. The cut objective does favor the true clusters over the ones generated by the random labels. 

#### B.2 The Volume Term 

Next, based on what we learnt above about the *volume term*, we then can write a function called `vols(A,y)` which computes the volumes of $$C_0$$ and $$C_1$$. The function should return a tuple, `v0, v1`, where `v0` is holding the volume of cluster `0` and `v1` holding the volume of cluster `1`. With the two volumes, we can apply them into a function named `normcut(A,y)`, which helps compute the binary normalized cut objective of a matrix `A` with clustering vector `y`. 


```python
def vols(A, y):
    """
    This function assumes that the input matrix A contains information about which points are near
    Its outputs are the the volumes of C_0 and C_1.
    """
    # get the volumes of C_0
    v0 = np.sum(A[y == 0.0])
    # get the volumes of C_1
    v1 = np.sum(A[y == 1.0])
    return v0, v1
```


```python
def normcut(A,y):
    # compute the volumes of C_0 and C_1
    v0, v1 = vols(A,y)
    # return the binary normalized cut objective
    return cut(A, y) * ( (1 / v0) + (1 / v1) )
```

Similarily, let's also do a comparsion of the `normcut` objectives using both the true labels `y` and the random labels generated above. My expectation is still that the normalized cut objective generated by true labels is the smaller one.


```python
normcut(A,y), normcut(A,ran_y)
```




    (0.02303682466323045, 2.0480047195518316)



As can be seen, the norm-cut computed by the fake labels is relatively larger than the one computed by the true labels `y`. It does matches what I expected.

## Part C

After defining a normalized cut objective which takes small values when the input clusters are (a) joined by relatively few entries in $A$ and (b) not too small, our next approach is to try to find a cluster vector `y` such that `normcut(A,y)` is small. However, this is an NP-hard combinatorial optimization problem. More details can be viewed [here](https://en.wikipedia.org/wiki/Combinatorial_optimization). To outcome the difficulty, we just need some math tricks! 

Here's the trick: define a new vector $$\mathbf{z} \in \mathbb{R}^n$$ such that: 

$$
z_i = 
\begin{cases}
    \frac{1}{\mathbf{vol}(C_0)} &\quad \text{if } y_i = 0 \\ 
    -\frac{1}{\mathbf{vol}(C_1)} &\quad \text{if } y_i = 1 \\ 
\end{cases}
$$

**Note: the `i` here refers to each data point. The signs(+/-) of the elements of $$\mathbf{z}$$ contain all the information from  y in terms of the clustering labels.**

Next, in linear algebra, there is a formula:

$$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2*\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;,$$

where $$\mathbf{D}$$ is the diagonal matrix with nonzero entries $$d_{ii} = d_i$$, and  where $$d_i = \sum_{j = 1}^n a_i$$ is the degree (row-sum) from before.  

Using the formula above, we can write a function called `transform(A,y)` to compute the appropriate $$\mathbf{z}$$ vector given `A` and `y`. Then, I'll check the equation above that relates the matrix product to the normcut objective by holding each side separately and also checking the equality.

#### Programming Note

To compute $$\mathbf{z}^T\mathbf{D}\mathbf{z}$$ in python, we can simply do `z@D@z`.


```python
def transform(A, y):
    # compute the volumes of C_0 and C_1
    v0, v1 = vols(A,y)
    # generate an empty array with the same length as y
    z = np.zeros(len(y))
    z[y == 0] = 1 / v0
    z[y == 1] = -1 / v1
    return z
```


```python
z = transform(A, y)
# construct a matrix D where its entries are zero
D = np.zeros(A.shape)
# let the diagonal entries of D equal to the degree (row-sum) from before
np.fill_diagonal(D, A.sum(axis = 0))
```

Here, we can also check the identity $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$ demonstrated as the following. Note: The $$\mathbb{1}$$ is the vector of `n` ones (i.e. `np.ones(n)`). This identity effectively says that $$\mathbf{z}$$ should contain roughly as many positive as negative entries. 


```python
z@D@np.ones(len(y)) == 0
```




    True



#### Note

Noticing that the equation $$\mathbf{N}_{\mathbf{A}}(C_0, C_1) = 2*\frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}}\;$$ is exact, we can apply `np.isclose(a,b)` to check if the left hand side is "close" to the right hand side of the equation. The following is the demonstration:


```python
LHS = normcut(A,y)
RHS = 2 * ( z@(D-A)@z ) / (z@D@z)
np.isclose(LHS,RHS)
```




    True



## Part D

In the last part, we saw that the problem of minimizing the normcut objective is mathematically related to the problem of minimizing the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. It's actually possible to bake this condition into the optimization, by substituting for $$\mathbf{z}$$ the orthogonal complement of $$\mathbf{z}$$ relative to $$\mathbf{D}\mathbf{1}$$. In the code below, I define an `orth_obj` function which handles this.

Use the `minimize` function from `scipy.optimize` to minimize the function `orth_obj` with respect to $$\mathbf{z}$$. Note that this computation might take a little while. Explicit optimization can be pretty slow! Give the minimizing vector a name `z_min`. 


```python
def orth(u, v):
    return (u @ v) / (v @ v) * v

e = np.ones(n) 

d = D @ e

def orth_obj(z):
    z_o = z - orth(z, d)
    return (z_o @ (D - A) @ z_o)/(z_o @ D @ z_o)
```


```python
import scipy.optimize
z_min = scipy.optimize.minimize(orth_obj, z).x
```

**Note**: there's a cheat going on here! We originally specified that the entries of $$\mathbf{z}$$ should take only one of two values (back in Part C), whereas now we're allowing the entries to have *any* value! This means that we are no longer exactly optimizing the normcut objective, but rather an approximation. This cheat is so common that deserves a name: it is called the *continuous relaxation* of the normcut problem. More details can be viewed in [here](https://en.wikipedia.org/wiki/Relaxation_(approximation)).

## Part E

Recall that, by design, only the sign of `z_min[i]` actually contains information about the cluster label of data point `i`. Plot the original data, using one color for points such that `z_min[i] < 0` and another color for points such that `z_min[i] >= 0`. 

My guess of the plot based on `z_min[i] < 0` will look much closer to the actual clustering.


```python
plt.scatter(X[:,0], X[:,1], c = z_min < 0)
```




    <matplotlib.collections.PathCollection at 0x2916935eeb0>




    
![output_37_1.png]({{ site.baseurl }}/images/output_37_1.png)
    


It seems like using `0` here as the threshold cannot do the job well. When I checked the `z_min`, almost every value is negative, which makes me concern about the potenital optimization issues. Then, behind the screen, I personally try a couple of threshold values. The one that give me more reasonable result is using `z_min < -0.00142`. Here's the demonstration:


```python
plt.scatter(X[:,0], X[:,1], c = z_min < -0.00142)
```




    <matplotlib.collections.PathCollection at 0x291693cacd0>




    
![output_39_1.png]({{ site.baseurl }}/images/output_39_1.png)
    


## Part F

Explicitly optimizing the orthogonal objective is  *way* too slow to be practical. If spectral clustering required that we do this each time, no one would use it. 

The reason that spectral clustering actually matters, and indeed the reason that spectral clustering is called *spectral* clustering, is that we can actually solve the problem from Part E using eigenvalues and eigenvectors of matrices. 

Recall that what we would like to do is minimize the function 

$$ R_\mathbf{A}(\mathbf{z})\equiv \frac{\mathbf{z}^T (\mathbf{D} - \mathbf{A})\mathbf{z}}{\mathbf{z}^T\mathbf{D}\mathbf{z}} $$

with respect to $$\mathbf{z}$$, subject to the condition $$\mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$. 

The Rayleigh-Ritz Theorem states that the minimizing $\mathbf{z}$ must be the solution with smallest eigenvalue of the generalized eigenvalue problem 

$$ (\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{D}\mathbf{z}\;, \quad \mathbf{z}^T\mathbf{D}\mathbb{1} = 0$$

which is equivalent to the standard eigenvalue problem 

$$ \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A}) \mathbf{z} = \lambda \mathbf{z}\;, \quad \mathbf{z}^T\mathbb{1} = 0\;.$$

Why is this helpful? Well, $$\mathbb{1}$$ is actually the eigenvector with smallest eigenvalue of the matrix $$\mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. 

> So, the vector $$\mathbf{z}$$ that we want must be the eigenvector with  the *second*-smallest eigenvalue. 

From above, we then can construct the (normalized) *Laplacian* matrix of the similarity matrix $$\mathbf{L} = \mathbf{D}^{-1}(\mathbf{D} - \mathbf{A})$$. By computing the eigenvector named `z_eig` corresponding to its second-smallest eigenvalue, I then can use the sign of it as the color to plot the data again.


```python
# construct the Laplacian matrix of the similartiy matrix A
L = np.linalg.inv(D)@(D - A)
w, v = np.linalg.eig(L)
# find the index of the second-smallest eigenvalue
index = np.where(w.argsort() == 1)
# find the eigenvector corresponding to its second-smallest eigenvalue
z_eig = v[:,index]
```


```python
plt.scatter(X[:,0], X[:,1], c = z_eig < 0)
```




    <matplotlib.collections.PathCollection at 0x2916afadc70>




    
![output_42_1.png]({{ site.baseurl }}/images/output_42_1.png)
    


In fact, `z_eig` should be proportional to `z_min`, although this won't be exact because minimization has limited precision by default. 

## Part G

Finally, we can synthesize all the results above and combine them into one function called `spectral_clustering(X, epsilon)`. Let's just demonstrate that using the supplied data from the beginning of the problem.


```python
def spectral_clustering(X, epsilon):
    """
    This function takes two inputs: a 2d-array X and the distance threshold epsilon.
    Its output is an array of binary labels indicating whether data point i is in group 0 or group 1.
    
    The method of this function in spectral clustering do the followings:
    
    This function takes algorithm of spectral clustering to seperate the data points into two groups,
    returning an array of binary labels (either 0 or 1). By accepting the two inputs, a 2d-array X
    and the distance threshold epsilon, the function will synthesize all the steps that starts from
    constructing the similarity matrix, constructing the Laplacian matrix, and computing the
    eigenvector with second-smallest eigenvalue of the Laplacian matrix. Finally, return labels
    based on the corresponding eigenvector.    
    """
    # Construct the similarity matrix.
    A = construct_similarity_matrix(X, epsilon)
    # Construct the Laplacian matrix.
    # construct a matrix D where its entries are zero
    D = np.zeros(A.shape)
    # let the diagonal entries of D equal to the degree (row-sum) from before
    np.fill_diagonal(D, A.sum(axis = 0))
    L = np.linalg.inv(D)@(D - A)
    # Compute the eigenvector with second-smallest eigenvalue of the Laplacian matrix.
    w, v = np.linalg.eig(L)
    # find the index of the second-smallest eigenvalue
    index = np.where(w.argsort() == 1)
    # Return labels based on this eigenvector.
    return np.where(v[:,index] < 0, 0, 1)
```

## Part H

Once our function is defined successfully, we can run a few experiments by generating different data sets using `make_moons`. I'm curious about What happens when we increase the `noise`? For the following experiments, I will try different sample size of the data including 100, 500, and 1000.


```python
np.random.seed(1234)
n = [100, 500, 1000]
fig, ax = plt.subplots(1, 3, figsize=(15,5))
for i in range(len(n)):
    X, y = datasets.make_moons(n_samples=n[i], shuffle=True, noise=0.05, random_state=None)
    # get the labels using our spectral clustering function
    labels = spectral_clustering(X, epsilon = 0.4)
    ax[i].scatter(X[:,0], X[:,1], c = labels)
    ax[i].title.set_text("Clustering when n = {n}".format(n = n[i]))
```


    
![output_47_0.png]({{ site.baseurl }}/images/output_47_0.png)
    


From the plots above, we can see that our function works well in clustering using different sample sizes!

## Part I

We can continue to try my spectral clustering function on another data set -- the bull's eye! 


```python
n = 1000
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
plt.scatter(X[:,0], X[:,1])
```




    <matplotlib.collections.PathCollection at 0x291658dc790>




    
![output_50_1.png]({{ site.baseurl }}/images/output_50_1.png)
    


There are two concentric circles. Using `k-means` here also fails to do the clustering correctly. 


```python
km = KMeans(n_clusters = 2)
km.fit(X)
plt.scatter(X[:,0], X[:,1], c = km.predict(X))
```




    <matplotlib.collections.PathCollection at 0x29165a28d30>




    
![output_52_1.png]({{ site.baseurl }}/images/output_52_1.png)

    


But what about my function? This time, I'll try some experimentation here with the value of `epsilon`, including 0.1, 0.25, 0.5, 0.75, 1.0.


```python
epsilon = [0.1, 0.25, 0.5, 0.75, 1.0]
fig, ax = plt.subplots(1,5, figsize=(18,5))
X, y = datasets.make_circles(n_samples=n, shuffle=True, noise=0.05, random_state=None, factor = 0.4)
for i in range(len(epsilon)):
    # get the labels using our spectral clustering function
    labels = spectral_clustering(X, epsilon[i])
    ax[i].scatter(X[:,0], X[:,1], c = labels)
    ax[i].title.set_text("Clustering when epsilon = {e}".format(e = epsilon[i]))
```


    
![output_54_0.png]({{ site.baseurl }}/images/output_54_0.png)

    


From the plots above, we can see that our function works well in clustering when using epsilon around `0.5` as the distance threshold that help cluster our sample data into two groups correctly!
